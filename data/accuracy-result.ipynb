{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_JOBS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- SVM\n",
    "- KNN\n",
    "- Decision Tree\n",
    "- Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "from sklearn.metrics import classification_report, confusion_matrix \n",
    "from sklearn.svm import SVC \n",
    "from sklearn.model_selection import GridSearchCV \n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVMGridSearch(dataSetName, X_train, y_train, X_test, y_test):  \n",
    "    # defining parameter range \n",
    "    param_grid = {'C': [0.1, 1, 10, 100, 1000],  \n",
    "                  'gamma': [1, 0.1, 0.01, 0.001, 0.0001], \n",
    "                  'kernel': ['rbf', 'linear']}  \n",
    "\n",
    "    grid = GridSearchCV(SVC(), param_grid, refit = True, verbose = 1, n_jobs = N_JOBS) \n",
    "    # fitting the model for grid search \n",
    "    grid.fit(X_train, y_train)\n",
    "    data = {}\n",
    "    data['dataSet'] = dataSetName\n",
    "    data['Algorithm'] = \"SVC\"\n",
    "    data['best_estimator'] = str(grid.best_estimator_)\n",
    "    data['training_score'] = grid.score(X_train, y_train)\n",
    "    data['testing_score'] = grid.score(X_test, y_test)\n",
    "    data['best_score'] = grid.best_score_\n",
    "    return data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KNNGridSearch(dataSetName, X_train, y_train, X_test, y_test):  \n",
    "    # defining parameter range \n",
    "    param_grid = {'n_neighbors': [3, 5, 7, 11, 19],  \n",
    "                  'weights': ['uniform', 'distance'], \n",
    "                  'metric': ['euclidean', 'manhattan']}  \n",
    "\n",
    "    grid = GridSearchCV(KNeighborsClassifier(), param_grid, refit = True, verbose = 1, n_jobs = N_JOBS) \n",
    "    # fitting the model for grid search \n",
    "    grid.fit(X_train, y_train)\n",
    "    data = {}\n",
    "    data['dataSet'] = dataSetName\n",
    "    data['Algorithm'] = \"KNN\"\n",
    "    data['best_estimator'] = str(grid.best_estimator_)\n",
    "    data['training_score'] = grid.score(X_train, y_train)\n",
    "    data['testing_score'] = grid.score(X_test, y_test)\n",
    "    data['best_score'] = grid.best_score_\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TreeGridSearch(dataSetName, X_train, y_train, X_test, y_test):  \n",
    "    # defining parameter range \n",
    "    param_grid = {'criterion': ['entropy', 'gini'], 'max_depth': [2, 5, 10, 12], 'min_samples_leaf': [2, 5, 10, 12]}\n",
    "\n",
    "    grid = GridSearchCV(DecisionTreeClassifier(), param_grid, refit = True, verbose = 1, n_jobs = N_JOBS) \n",
    "    # fitting the model for grid search \n",
    "    grid.fit(X_train, y_train)\n",
    "    data = {}\n",
    "    data['dataSet'] = dataSetName\n",
    "    data['Algorithm'] = \"DecisionTree\"\n",
    "    data['best_estimator'] = str(grid.best_estimator_)\n",
    "    data['training_score'] = grid.score(X_train, y_train)\n",
    "    data['testing_score'] = grid.score(X_test, y_test)\n",
    "    data['best_score'] = grid.best_score_\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BayesSearch(dataSetName, X_train, y_train, X_test, y_test):  \n",
    "    grid = GaussianNB()\n",
    "    grid.fit(X_train, y_train)\n",
    "    data = {}\n",
    "    data['dataSet'] = dataSetName\n",
    "    data['Algorithm'] = \"Naive Bayes\"\n",
    "    data['best_estimator'] = \"GaussianNB()\"\n",
    "    data['training_score'] = grid.score(X_train, y_train)\n",
    "    data['testing_score'] = grid.score(X_test, y_test)\n",
    "    data['best_score'] = data['training_score']\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.ipynb_checkpoints',\n",
       " 'accuracy-result.ipynb',\n",
       " 'cleaned-data',\n",
       " 'details',\n",
       " 'done-cleaning-data',\n",
       " 'done-results-data',\n",
       " 'pipeline',\n",
       " 'results',\n",
       " 'results-metadata',\n",
       " 'to_sahil',\n",
       " 'to_sahil.zip',\n",
       " 'transfusion.csv',\n",
       " 'Untitled.ipynb']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data_banknote_authentication.csv']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('cleaned-data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting.. data_banknote_authentication-scored\n",
      "{'dataSet': 'data_banknote_authentication-scored', 'Algorithm': 'Naive Bayes', 'best_estimator': 'GaussianNB()', 'training_score': 0.8464528668610302, 'testing_score': 0.8279883381924198, 'best_score': 0.8464528668610302, 'TimeTaken': 0.002995014190673828, 'njobs': 3}\n",
      "Fitting 5 folds for each of 32 candidates, totalling 160 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done 140 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=3)]: Done 160 out of 160 | elapsed:    1.5s finished\n",
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataSet': 'data_banknote_authentication-scored', 'Algorithm': 'DecisionTree', 'best_estimator': 'DecisionTreeClassifier(max_depth=10, min_samples_leaf=2)', 'training_score': 0.9931972789115646, 'testing_score': 0.9766763848396501, 'best_score': 0.9825147999052806, 'TimeTaken': 1.5910186767578125, 'njobs': 3}\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "{'dataSet': 'data_banknote_authentication-scored', 'Algorithm': 'KNN', 'best_estimator': \"KNeighborsClassifier(metric='euclidean', n_neighbors=3)\", 'training_score': 0.9990281827016521, 'testing_score': 1.0, 'best_score': 1.0, 'TimeTaken': 0.17552947998046875, 'njobs': 3}\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n"
     ]
    }
   ],
   "source": [
    "folder = 'cleaned-data'\n",
    "files = os.listdir('cleaned-data')\n",
    "\n",
    "for file in files:\n",
    "    df = pd.read_csv(folder+\"//\"+file)\n",
    "    dataset_name = file.split(\".\")\n",
    "    dataset_name = dataset_name[:-1]\n",
    "    dataset_name = ''.join(dataset_name)\n",
    "    dataset_name += '-scored'\n",
    "    X  = df.iloc[:,:-1].values\n",
    "    y  = df.iloc[:,-1:].values\n",
    "    y = y.ravel()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "    print(\"starting..\", dataset_name)\n",
    "    \n",
    "    start = time.time()\n",
    "    data = BayesSearch(dataset_name ,X_train, y_train, X_test, y_test)\n",
    "    timeTaken = time.time() - start\n",
    "    data['TimeTaken'] = timeTaken\n",
    "    data['njobs'] = N_JOBS\n",
    "    json_file_name = \"results/\" + dataset_name + \"-bayes.json\"\n",
    "    print(data)\n",
    "    with open(json_file_name, \"w\") as f:\n",
    "        json.dump(data, f)\n",
    "        \n",
    "    start = time.time()\n",
    "    data = TreeGridSearch(dataset_name ,X_train, y_train, X_test, y_test)\n",
    "    timeTaken = time.time() - start\n",
    "    data['TimeTaken'] = timeTaken\n",
    "    data['njobs'] = N_JOBS\n",
    "    json_file_name = \"results/\" + dataset_name + \"-tree.json\"\n",
    "    print(data)\n",
    "    with open(json_file_name, \"w\") as f:\n",
    "        json.dump(data, f)\n",
    "    \n",
    "    start = time.time()\n",
    "    data = KNNGridSearch(dataset_name ,X_train, y_train, X_test, y_test)\n",
    "    timeTaken = time.time() - start\n",
    "    data['TimeTaken'] = timeTaken\n",
    "    data['njobs'] = N_JOBS\n",
    "    json_file_name = \"results/\" + dataset_name + \"-knn.json\"\n",
    "    print(data)\n",
    "    with open(json_file_name, \"w\") as f:\n",
    "        json.dump(data, f)\n",
    "        \n",
    "    start = time.time()\n",
    "    data = SVMGridSearch(dataset_name ,X_train, y_train, X_test, y_test)\n",
    "    timeTaken = time.time() - start\n",
    "    data['TimeTaken'] = timeTaken\n",
    "    data['njobs'] = N_JOBS\n",
    "    json_file_name = \"results/\" + dataset_name + \"-svm.json\"\n",
    "    print(data)\n",
    "    with open(json_file_name, \"w\") as f:\n",
    "        json.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
